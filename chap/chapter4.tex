% Also generell geht es in diesem Kapitel ja um das phantom Plugin an sich. Ich habe mich 
% aber entschieden das so aufzubauen, dass ich die zwei Sachen die ich gemacht habe 
% besonders in einer eigenen section beleuchten will.
% Das heißt der ganze Rest muss in dieses erste Kapitel
% Dabei will ich zunächst dasrtellen, welche Anforderungen and die Kamera gestellt werden 
% auch was dass Libuca interface angeht. Dann will ich klarstellen, was Matthias schon 
% gemacht hatte und dann sagen was ich alles gemacht habe.
\section{Overview}

% Hier will ich nochmal aufzählen, was ich alles machen muss auch was die implementation 
% des Interface von Libuca angeht.
% Ich denke ich sollte hier eine richtige Einführung von LibUCA geben. Ich meine klar 
% ich habe in der Introduction schon was dazu geschrieben, aber das war ja mehr so ein 
% Verkaufstext. Daraus geht ja nicht wirklich hervor, wie das Ding tatsächlich 
% funktioniert. Aber in welcher Reihenfolge?
% 1) Nochmal eine kleine Verkaufszusammenfassung
% 2) Wie LibUCA von aussen funktioniert also, wie man es benutzt (die wichtigsten 
% Funktionen und property access)
% 3) (Python Binding)
% 4) Wie man das mit dem Plugin macht.
\subsection{LibUCA}
% Hier eine generelle Einführung, wie man Libuca in seiner eigenen C application benutzen
% würde.
% ICH DENKE HIER KÖNNTE ICH NOCH AUSFÜRHLICHER SCHREIBEN.
\paragraph{Using the LibUCA library}
The \textit{Unified Camera Access Library (LibUCA)} was designed to provide a universal interfacing tool for switching between various research cameras with minimal effort. The LibUCA API consists of a set of basic camera operations, such as starting a recording or grabbing a frame. These operations can be used in the codebase of a scientific project. Integrating a new camera would only require to implement these methods once in the form of a LibUCA plugin. Switching to this camera then would only require to change parameters in the existing code.\par
After the library has been installed properly, it can be used as part of a C application by including the three header files \texttt{glib-object.h}, \texttt{uca/uca-plugin-manager.h} and \texttt{uca/uca-camera.h}.\\
A new \texttt{UcaPluginManager} object has to be created. This plugin manager is required to create a new camera object from one of the installed plugins. The camera type is identified by a string, which is passed to the plugin manager:
\lstset{language=C}
\begin{lstlisting}
#include <glib-object.h>
#include <uca/uca-plugin-manager.h>
#include <uca/uca-camera.h>

int main (int argc, char *argv[])
{
	// Creating camera object
	UcaPluginManager *manager;
	UcaCamera *camera;
	GError **error = NULL;
	
	g_type_init();
	
	manager = uca_plugin_manager_new();
	camera = uca_plugin_manager_get_camera(manager, "mock", &error);
\end{lstlisting} 
For testing purposes, the "mock" plugin comes installed with the main library. This camera object creates random frames upon calling the "grab" function.\par
The configuration of the camera parameters is done by getting and setting properties of the camera object:
\begin{lstlisting}
	// Setting properties	
	guint exposure_time = 0.01;
	guint frame_rate = 1000;
	
	g_object_set( G_OBJECT(camera),
		"exposure-time", exposure_time,
		"frame-rate", frame_rate,
		NULL,							// NULL marks the end		 
	);	
		
	// Getting properties
	guint frame_height;
	guint frame_width;
	
	g_object_get( G_OBJECT(camera),
		"roi-width", &frame_width,
		"roi-height", &frame_height,
		NULL,
	)	
\end{lstlisting}
Most cameras share common properties such as exposure time and frame rate. Specific camera models can have more exotic configuration options, which have to be listed in the corresponding plugin documentation.\\
To acquire frames from the camera, a recording has to be started first. The "grab" method of the camera object is used to get the frames. The image data is saved into a buffer, which is also passed as a parameter to the method. The frames will always be in 16bit format, which is 2 bytes of data per pixel. It is important that the buffer size is setup correctly with the resolution of the camera.\\
Cameras with an internal memory can be triggered. This means they will save a set amount of frames into their internal memory after a trigger event has occurred. Some cameras support software triggering. For this case a libUCA plugin can implement the "trigger" method, which will send a software trigger command to the camera:
\begin{lstlisting}
	// Setting up the buffer with size in BYTES
	gpointer buffer = g_malloc0( frame_height * frame_width * 2 );
	
	// The recording
	uca_camera_start_recording(camera, &error);

	uca_camera_grab(camera, buffer, &error);
	uca_camera_trigger(camera, &error);
			
	uca_camera_stop_recording(camera, &error);
	
	// Readout of the triggered frames...
}
\end{lstlisting}
% So als nächstes will ich eine section machen dazu wie man ein Plugin designen muss. 
% Also welche requirements man dazu braucht.
\paragraph{Requirements for a LibUCA plugin}
Since LibUCA is using the \texttt{UcaCamera} object to interact with the actual cameras, a new camera plugin has to create a sub class of \texttt{UcaCamera}. As a first step this includes the necessary functions to initialize the camera object and the getting/setting of its properties. This follows the general sub-classing principles of GLib. Although it is advised to use the source code for the "mock" plugin as a template, as the generic initialization tasks are already implemented [CITATION FOR THIS SUBCLASSING].\par 
For minimal working plugin the following three methods have to be implemented for the camera:
\begin{itemize}
\item \texttt{start\_recording}: This method has to make all necessary preparations so that every subsequent call to the \texttt{grab} method returns an image array.

\item \texttt{stop\_recording}: Stops the recording so that subsequent calls to the grab method fail.

\item \texttt{grab}: Has to return an image in the form of an array, where each pixel is represented by a 16 bit integer value.
\end{itemize}
Additionally for cameras with an internal memory, the "trigger" method and the readout mode can be implemented:
\begin{itemize}
\item \texttt{start\_readout}: This method has to make all necessary preparations so that frames from the cameras memory can be read.

\item \texttt{stop\_recording}: Stops the readout.

\item \texttt{trigger}: Issues a software trigger command within the camera, so that it will store a previously defined amount of frames in its internal memory.
\end{itemize}

% Hier sollte eine kleine Zusammenfassung rein von den features, die Marcus angesprochen 
% hat, die wichtig wären.
\subsection{requirements}


% Hier sollte ich rein schreiben, was genau Matthias schon alles gemacht hat.
\subsection{previous work}
The development of the uca-phantom plugin was originally started by Matthias Vogelgesang, who is also the author of the LibUCA library. He implemented most of the basic features:
\paragraph{\texttt{PhantomCamera} class}
% Matthias hat die das ganze GLIB setup mit init und so gemacht
First of all he implemented the basic GLib structure for the \texttt{PhantomCamera} sub class. This includes the method for creating, initializing and tearing down the camera object. He also added some basic properties to control the camera behaviour, which include the exposure time, frame rate, post trigger frames among others. Upon taking over the project, the \texttt{PhantomCamera} object could be created without any errors.
\paragraph{Getting and setting properties}
% Matthias hat getting und setting schon fertig implementiert 
Furthermore Matthias implemented the control communication with the phantom camera: Upon construction of the camera object the phantom discovery protocol is used to acquire the IP address of the camera. A new TCP socket is setup with this IP address and is saved as an attribute of the camera object. The \texttt{phantom\_get} method is used to send a \textit{get} command (sec.\ref{sec:phgetset}) to the camera using the TCP socket. The response is then properly parsed and the value is returned. The \texttt{phantom\_set} method is used to send a \textit{set} (sec.\ref{sec:phgetset}) command, to modify the cameras internal properties.\\
Finally these methods haven been used to map the properties of the \texttt{PhantomCamera} object to the actual internal properties of the phantom camera. So simply setting a property on the object would invoke the method, which sends a set command to the camera over the network.
\paragraph{1G frame transmission}
% Matthias hat auch das ganze Ding mit normal 1G auslesen implementiert
The grab function was also already implemented and working. Due to the phantoms requirement to open a secondary connection for data transmission (sec.\ref{sec:phreadout}), the plugin implementation had to include a threaded architecture for this additional connection. During the start\_recording function a new listening TCP socket is created, as well as a new Thread. The different threads are communicating using an asynchronous message queue. Within this new data thread the socket will accept the incoming connection from the camera. The main loop of the thread will wait until a new message has been put into the async queue. After a message of the type "read\_image" is fetched from the queue, the thread will start to receive the data from the listening socket and put a new \texttt{Result} into the message queue.\\
By using this threaded structure, the grab function only has to send the \textit{img} command (sec.\ref{sec:phreadout}) to the camera, put a "read\_image" message into the message queue and wait to fetch a result from the queue.
[ILLUSTRATION ABOUT THE WHOLE STRUCTURE WITH THE THREAD AND QUEUE]

% Hier schreibe ich dann rein, was ich gemacht habe. Also denke mal so eine kleine 
% Zusammenfassung der kleineren Themen, die ich noch angegangen bin und einen Zusam.
% der nächsten zwei Kapitel
\subsection{additional work}
Building on the previous work, I had to implement the following features:
\begin{itemize}
\item High speed data transmission using the 10G ethernet interface
\item The triggering and the readout of memory data
\item The memory gate function for the auxiliary port
\end{itemize}
% Hier noch schreiben, dass high speed seine eigenen sections bekommt, die anderen aber eher kurze Aufgaben waren

% Hier will ich den memory mode erklären, also wie ich einen pointer gemacht habe auf 
% die letzte memory section etc.
\paragraph{the memread mode}

% Dazu gibt es eigentlich nicht zu viel zu sagen, aber ich könnte schreiben, wie ich 
% drauf gekommen bin, was ich da rein schreiben sollte.
% composite deshalb schwerer
% leider keine Doc, deshalb musste ich ausprobieren.
\paragraph{memory gate function}

% In diesem Abschnitt soll es darum gehen, wie die 10G Übertragung funktioniert. 
% Dazu will ich es unterteilen in eine kleinen Theorieteil draüber wie raw ethernet 
% frames aussehen und wie die Daten generell von der Phantom übertragen werden. 
% Dann will ich generell das Setup von packet_mmap erklären und dann die spezielle 
% Implementierung in meinem Programm.
\section{High speed ethernet transmission}

% Hier sollte ich reinschreiben, was denn genau das Problem ist, also was ich ausgetestet 
% habe und auch genau definieren welche Daten im 10G Modus bei mir ankommen und was der 
% Unterschied ist zu normaler Übetragung. Vielleicht auch noch eine formelle Definition 
% von raw ethernet frames und wie TCP darauf aufgebaut ist.
\subsection{The problem}

The 1G data transmission for the phantom camera is implemented using the TCP protocol (sec.\ref{sec:tcp}). Thus the interface on the software side is quite straight forward: On the receiving end, a listening TCP socket is being established. The camera will connect to this socket and data is being transmitted using this TCP channel. On the programming side, this can be compared to working with a file.\\
But with the ease of a TCP implementation of the data transmission there comes a significant speed loss. On the one hand using TCP limits the usable payload size per used frame, as both the TCP (transport layer) and the IP (network layer) define additional headers. On the other hand always waiting for a reply from the server limits the potential capacity for an inherently one-way data transmission a lot.\par 

Due to these disadvantages, the 10G data transmission of the phantom camera does not use the TCP protocol. Instead it directly uses ethernet frames to pack the data stream into. Ethernet frames are "lower level" protocol from the data link layer (sec.\ref{sec:eth}). Using a lower level protocol increases the speed by reducing the overhead. But increased transmission speed comes with the loss of the ease, which TCP provides.\par 

My first attempt at implementing the data reception was to apply the classic approach: Setting up a receiving socket for raw ethernet frames and then using a loop to read from it.\\
Using this approach showed a big problem which had to be overcome. Due to the camera not having to wait for a response of the server, all the data of an image was being \textit{dumped} to the ethernet connection in rapid succession. On the receiving end the \texttt{read} function could not be called fast enough to receive all the data. During the time, it took the application process to empty the small internal buffer of the \texttt{read} function, multiple frames would have passed unnoticed. Depending on the used hardware, only about 10 to 30 percent of the data could be received with this approach.\\
So the challange was to implement the reception of the data in a different manner, so that the full image data could be received reliably.

[ILLUSTRATION OF THE PROBLEM ?]

% Hier will ich nur das allgemeine zeug zu packet_mmap reinschreiben. Also welche 
% unterschiedlichen Elemente es gibt
\subsection{Using \texttt{packet\_mmap}}
\label{sec:mmap}

% PUT A BARE BONES SETUP EXAMPLE OF PACKET_MMAP INTO APPENDIX?

\texttt{PACKET\_MMAP} is a networking library available on Linux distributions.\\
On default, the capture process using a regular network socket uses limited buffer size and requires one system call per packet to be captured. \texttt{packet\_mmap} on the other hand is much more efficient. It provides a size configurable circular buffer. Once setup all packets read by the network card of the machine will be automatically written into this buffer, without the need for any additional system calls. Additionally the buffer is directly mapped into the user space. This means, that application software can directly read from this kernel-level buffer without the need for additional copying operations.\\
The ring buffer is configurable to a much higher storage capacity than traditional socket buffers. This means, that a single image can easily be received without data loss.\cite{packetmmap}\\
% How do you work with packet mmap on the programming side?
To use \texttt{packet\_mmap} as part of an application to capture packets, there are three steps in the lifecycle of the read process:

\begin{description}
\item[setup] The setup process roughly consists of theses function calls: \texttt{socket()} creates the capture socket. \texttt{setsocketopt()} is used to control the allocation of the circular buffer. Here the given options set the frame size, the block size  and count. These properties determine the overall buffer size. \texttt{mmap()} maps the allocated buffer to the user process of the application
\item[capture] In general the buffer is being filled with incoming packets automatically. Within the application \texttt{poll()} is being used to wait until a new packet has been written to the buffer.
\item[shutdown] At the end of the program the call of \texttt{close()} will issue the destruction of the capture socket and the deallocation of all associated resources.
\end{description}

For a minimal working example of a \texttt{packet\_mmap} capture application see the appendix A [WRITE THE APPENDIX].\par 

\paragraph{The ring buffer}

the packets received by the network socket are automatically stored within a ring buffer. The size of this buffer is configurable. The ring buffer consits of $m$ \textit{blocks}, each block consists of $n$ frames with the size $s_{frame}$. Thus the \text{total size}\footnote{For this application it is important to note, that this size cannot be used to calculate the total amount of images. The ring buffer holds the complete ethernet packets, which includes a small overhead caused by the additional header.} of the buffer is calculated as:
\[
s_{total} = m \cdot n \cdot s_{frame}
\]
Access to the ring buffer from within the application is done block wise. data within a block cannot be accessed, while the kernel is currently writing to this block. Only after a block has been filled with the received packets, it is flagged as accessible by the user space. After an application has read all the data from this block, the block needs to be explicitly flagged as kernel space again. This is because a block that has been marked as user space will not be written by the kernel. This fact creates a special dependency for the application program: The blocks need to be read faster by the application than the kernel can fill them. If this is not the case, data loss might occur, because at some point all blocks will me marked as user space and the kernel cannot write new incoming packets.\\
Accessing the packets within these packets is given as a pointer to the start of each block. The packet space within these blocks is allocated next to each other in memory. To move on to the next packet, the pointer has to be incremented by the size of one packet.

% Description of how to deal with the buffers programmatically.

[ILLUSTRATION OF THE BUFFER]

% Ein architectural overview geben, wie ich dann mein Programm aufgebaut habe mit dem 
% separaten decoding algorithmus in einem Thread und wie die Pointer zwischen den 
% verschiedenen Methoden calls hin und her gereicht werden.
\subsection{The implementation}
\label{sec:architecture}

\paragraph{architectural overview}

As for the overall architecture of the program, the first attempt was to use a simple serial processing of the image data: The \texttt{grab()} function called on the camera object would send an \texttt{img} command string to the camera, requesting a single image. This image data would be sent as multiple ethernet frames, written into the circular buffer. From the circular buffer the data was being read and then decoded.\\
It became quickly apparent though, that even with the usage of \texttt{packet\_mmap} this serial design would not meet the desired transmission speed. The process of receiving the image data consisted of multiple steps: First the data being requested, then the data being received and finally decoded into the desired format.

[ILLUSTRATION OF THIS SERIAL PROCESS IN COMPARISON TO PARALLEL]

Due to a lot of the steps of this process being idle during the processing of one image, the design was changed to make the individual steps run parallel to each other.\\
The first change was made to the process of requesting image data from the camera. Given the common case, that $n$ images have to be read from the camera, they are being requested in chunks. A single \texttt{ximg} request will issue the camera to send up to one hundred images at the same time.\\
Using \texttt{packet\_mmap} these images will be written into the ring buffer. Within the application there is a thread running, which reads the raw ethernet packets from this ring buffer. This thread identifies the payload of each packet and collects all the packets to form a single image. This image is then copied into a global intermediate buffer. After one image has been copied, the thread immediately starts to collect data for the next image.\\
Yet another thread is responsible for the image decoding. This thread waits for images being written to the global buffer. Once an image has been written to it, it starts to decode the transfer format into the final 16 bit format. After the decoding process is done, the final image will be written into the global output buffer, which is used by the libuca framework to yield the return for the \texttt{grab} function.\\
Using this basic design, the different steps in receiving image data are able to run in parallel with reduced idle time, thus increasing data throughput.

\paragraph{Implementation of image reception}

The raw ethernet packets are being received and written into the ring buffer automatically. Within the application these packets are then being read by a separate thread.\\
The main challange here is to identify all those bytes, which belong to one image. When requesting the transmission of 100 images for example, the camera will take the bits of all these images concatenate them and send this big sequence of bytes in multiple small ethernet packets, without reference to where one image ends and another begins.\\
The core problems encountered with this procedure are the following:

\begin{description}
\item[dynamic image size] The size of one image can vary to great extend, based on the camera settings. The amount of pixels itself is based on the configurable resolution of the camera, as well as the height and width of the region of interest. Additionally the size of the byte stream varies depending on the configured transfer format. Currently there are a 10 bit and a 12 bit supported for 10G data transmission.\\
\item[byte packing] Since the image data of multiple images is being concatenated and sent over the network as one big byte stream, there are no natural indications where one frame ends and the other begins. Usually it is the case, that the transition of one image to another is within the payload of one packet.\\
\end{description}

To solve these problems the reception process of each frame starts with a calculation of the expected size of one image, based on the resolution and used transfer format.\\
As mentioned in section \ref{sec:mmap} access to the ring buffer is only available block wise. Since the application reads data faster, than the kernel is writing it, the main loop of the receive thread is polling for the next block to be freed to user space. Within each block, the packets are processed in series. While processing the packets the application keeps a counter of how many bytes have been extracted already. If this count reaches the calculated expected size of one image, the data is copied into the global buffer to be decoded. The counter is being reset to track the next image.  

% Hier soll es um die decoding algorithmen gehen. Und zwar angefangen mit dem Problem 
% statement, also "Ich habe gemerkt dass nachdem die 10G übertragung funktioniert hat,
% das decoding das eigentliche bottleneck war. Dann eine Einführung darin was SSE ist 
% und wie es mein Problem lösen kann. Und dann meine tatsächlichen Algorthmen beschreiben 
% die 12 und 10 Bit decodieren.
\section{Optimized Decoding Algorithms}

% Nochmal aus dem oberen Kapitel darauf zurückkommen, dass es einen decoding thread geben 
% soll. Wie ich versucht habe den normalen Algorithmus zu benutzen und wie der so gar
% nicht funktioniert hat also ein super speed bottle neck war.
% Dann das Format des encodings erklären.
\subsection{The problem}

As explained in the section \ref{sec:architecture}, to meet the transmission speed goals, the process of receiving images has been parallelized. The thread reading from the ring buffer will assemble the bytes of a single image and then write those into a global buffer. From this global buffer the decoding thread will read the data, decode the image and write the final data into the output buffer, which passes the data to be returned by the call to the \texttt{grab} function.\par 

The libuca framework specifies, that the grab function will return images, where each pixel is described by 16 bit number. The phantom camera protocol only offers transfer formats, which represent pixels either with 10 or 12 bit.\\
Since the phantom camera has a 12 bit sensor depth, the 12 bit format will transmit the images just the way they are stored within the camera. The 10 bit format on the other hand is a form of loss-based compression. The camera will cut the 2 least significant bits from each pixel. This format can be used for even higher data transmission rates, due to the $16,67\%$ reduction of total transfer volume.\\
Either way, in the end each image has to be converted, such that each pixel is represented by 16 bit. This conversion is simply achieved by appending multiple zero's to the bytes of each pixel.

[ILLUSTRATION HOW EACH PIXEL IS INFLATED TO 16 BIT]

A first, simple approach to achieve this conversion is by simply iterating over each pixel in the input buffer, copying the value together with the additional bits into the output buffer.\\
Using this approach presents a major problem to the transmission speed. A traditional approach using a simple loop structure, even with additional optimizations, does not satisfy the requirements.

% Eine generelle Einführung zum SSE instruction set und wie es mir helfen kann mein 
% problem zu lösen. Vielleicht einen Paragraphen "ausgewählte Befehle", wo ich die 
% erkläre, die ich tatsächlich gebraucht habe.
% THIS SECTION SHOULD BE GOING TO THE BACKGROUND CHAPTER, AT LEAST A GENERAL EXPLANATION OF WHAT IT IS
\subsection{SSE Instruction set}

Due to the the traditional approach failing the speed requirements, a different approach was needed. As introduced in section \ref{sec:simd}, SIMD instruction are able to provide a significant performance advantage in time critical applications, due to their inherently concurrent execution within the processor.\\
% Explain what SSE is specifically
The \textit{Streaming SIMD Extension (SSE)} was originally introduced with Intels Pentium 3 processor. SSE adds both new register types as well as new instructions. The capabilities of using SIMD instructions was then expanded upon with each new version of the SSE standard up to the currently newest version SSE4.2 \cite{ModernAssembly}.\\
For the image decoding algorithms only instructions from the SSE2 version were used.\par

Using the SSE intrinsic functions, new decoding algorithms were implemented for the 10 bit and 12 bit transfer formats.\\
When using the SSE instructions it is important to note, that an algorithm, that was implemented with a traditional loop most of the time cannot be translated simply to SSE. The usage of the specifically optimized functions needs a rethinking of algorithm design.
% Maybe add a paragraph, which explains the simple sse instructions 

% Hier kommt dann die Erklärung der eigentlichen Algorithmen und der Pseudocode. In den 
% Algorithmen brauche ich dann die coolen Grafiken, die klar machen welche shift 
% operationen ich nacheinander mache. Die muss ich dann noch in Gimp erstellen...
\subsection{10Bit decoding}

The core property of working with SSE for this decoding problem is the usage of 128 bit integer \textit{vectors}, as they are called. Data from the image byte stream is loaded into these vectors and from them the data is saved to the output buffer as well.\\
with 128 bit wide vectors, the final calculation product will hold the data of 8 pixels:
\[
\frac{128\text{Bit}}{16 \text{Bit per Pixel}} = 8 \text{Pixels}
\]
This means, that in every step of the iteration over the image data 8 Pixels are being decoded \textit{concurrently}.\par 

In general the algorithm follows 3 steps:
\begin{description}
\item[Loading] The raw data from the image is being loaded into several SSE 128 bit vectors.
\item[Shifting] The data within these vectors is shifted in such a way to append 6 bit zeros at the end of each 10 bit pixel
\item[Saving] The shifted vectors are combined into a single vector holding the information of all the 8 pixels. The data from this vector is then appended to thee output buffer.
\end{description}

[ILLUSTRATION OF HOW THE ALGORITHM WORKS VISUALLY]

\paragraph{Loading}

So at the very beginning, the data from the image is being loaded into a single 128 bit vector. But since every pixel is only 10 bit, this vector initially stores information about $12,8$ pixels. Because the algorithm only processes 8 pixels in one step, the input vector is masked to only the first 80 bit. These 80 bit will later on be inflated to use up the whole 128 bit range of the vector.\\
The next step is to divide the data within this single input vector into 4 separate vectors. The final goal is to shift all 10 bit sections into respective 16 bit sections. So when looking at the required shifts to be made to the data, there are four different shifts required to different sections of the original data. To create these four separate vectors, the shuffle operation is used to roughly position these four sections of bits into the right offsets within the 128 bit range and then they are masked, so only the relevant bits for each shifting operation are contained in each of the vectors.

\paragraph{Shifting}

The next step is to actually perform the shifting operation on all of these vectors. One section does not have to be shifted at all, since the shuffle operation positioned it into the correct place right away. The other sections have to be shifted by 2, 4 and 6 bits.\\
The final product of the shifting stage is that there are now 4 vectors containing each 32 bits of image data, which are positioned into there final positions as 16 bit representations

\paragraph{Saving}

The last step is to take these 4 vectors and merge them into a single vector, which contains all the correct data. Since the vectors contain only zeros at the positions, where there is no actual pixel data, the vectors can be merged with a simple \texttt{bitwise OR} operation.\\
Since SSE only supports pairwise OR operations, there are three merges needed to obtain the final vector.\\
The data from this final vector is then appended to the output buffer. In the end the pointers for the input of the algorithm and the output of the algorithm are updated. The input pointer is incremented by 80, since 8 pixels of the 10 bit format were processed. The pointer to the position in the output buffer is incremented by 128, since that amount of new data has just been written to it.

% MAYBE WRITE APPENDIX WITH A WORKING EXAMPLE

\subsection{12 bit decoding}

The 12 bit transfer format decoding follows the same general steps as the 10 bit algorithm. With the only difference being, that the needed shifts and masks are now a little different. In fact the 12 bit algorithm only uses 2 vectors. Due to the superior symmetry of 12 bit towards 16 bit, there are only fundamentally different shifts required after shuffeling.\\
Thus the 12 bit algorithm also only requires 1 merge operation, which makes it slightly more efficient than the 10 bit counterpart.

[ILLUSTRATION OF HOW THE 12 BIT SHIFTS WORK?]

% Ich bin mir noch nicht ganz sicher aber ich denke die Benchmarks sollten ein eigenes 
% Kapitel haben. Dazu muss ich ja dann auch erstmal noch was programmieren...
%\subsection{Benchmarks}

